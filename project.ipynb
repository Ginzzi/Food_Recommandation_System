{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.callbacks import CSVLogger\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = open('keptcolumns.txt', 'r')\n",
    "keptcolumns = fi.read().rstrip('\\n').split(', ')\n",
    "keptcolumns = np.array(keptcolumns)\n",
    "data = pd.read_csv('epi_r_toy.csv', usecols = keptcolumns)\n",
    "# data = pd.read_csv('epi_r.csv', usecols = keptcolumns) # comment out if you want to use the original dataset \n",
    "data = data[pd.notnull(data['calories'])]\n",
    "data = data[pd.notnull(data['protein'])]\n",
    "data = data[pd.notnull(data['fat'])]\n",
    "data = data[data['calories'] != 0]\n",
    "data = data[data['protein'] != 0]\n",
    "data = data[data['fat'] != 0]\n",
    "data = data.values\n",
    "h_val = data[:, 0:6]\n",
    "ingred = data[:, 6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dat(xy):\n",
    "    cnt = np.sum(xy == 1)\n",
    "    idx = np.where(xy == 1)\n",
    "    x = np.zeros((cnt, xy.shape[0]))\n",
    "    y = np.zeros((cnt, xy.shape[0]))\n",
    "    for i in range(cnt):\n",
    "        y_idx = idx[0][i];\n",
    "        x_idx = np.setdiff1d(idx[0], y_idx)\n",
    "        y[i, y_idx] = 1\n",
    "        x[i, x_idx] = 1\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8673, 308)\n",
      "(8673, 308)\n"
     ]
    }
   ],
   "source": [
    "x = np.empty((0, ingred.shape[1]))\n",
    "y = np.empty((0, ingred.shape[1]))\n",
    "for i in range(ingred.shape[0]):\n",
    "    x_tmp, y_tmp = gen_dat(ingred[i])\n",
    "    x = np.append(x, x_tmp, axis = 0)\n",
    "    y = np.append(y, y_tmp, axis = 0)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6938, 308)\n",
      "(6938, 308)\n",
      "(1735, 308)\n",
      "(1735, 308)\n"
     ]
    }
   ],
   "source": [
    "idx = np.arange(x.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "p = x.shape[0] * 8 // 10\n",
    "x_train = x[:p, :]\n",
    "y_train = y[:p, :]\n",
    "x_test = x[p:, :]\n",
    "y_test = y[p:, :]\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               61800     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 308)               92708     \n",
      "=================================================================\n",
      "Total params: 485,708\n",
      "Trainable params: 485,708\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential([Dense(200, input_shape=(308,)),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(300),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(300),\n",
    "                                    LeakyReLU(),\n",
    "                                                                                                            \n",
    "                                    Dense(300),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(300),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(308, activation='sigmoid')\n",
    "                                   ])\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/22\n",
      "6938/6938 [==============================] - 1s 110us/step - loss: 5.2791 - categorical_accuracy: 0.0241\n",
      "Epoch 2/22\n",
      "6938/6938 [==============================] - 1s 83us/step - loss: 4.9686 - categorical_accuracy: 0.0231\n",
      "Epoch 3/22\n",
      "6938/6938 [==============================] - 1s 85us/step - loss: 4.7853 - categorical_accuracy: 0.0278\n",
      "Epoch 4/22\n",
      "6938/6938 [==============================] - 1s 88us/step - loss: 4.5922 - categorical_accuracy: 0.0329\n",
      "Epoch 5/22\n",
      "6938/6938 [==============================] - 1s 100us/step - loss: 4.3737 - categorical_accuracy: 0.0334\n",
      "Epoch 6/22\n",
      "6938/6938 [==============================] - 1s 90us/step - loss: 4.1749 - categorical_accuracy: 0.0412\n",
      "Epoch 7/22\n",
      "6938/6938 [==============================] - 1s 90us/step - loss: 3.9955 - categorical_accuracy: 0.0470\n",
      "Epoch 8/22\n",
      "6938/6938 [==============================] - 1s 92us/step - loss: 3.8207 - categorical_accuracy: 0.0597\n",
      "Epoch 9/22\n",
      "6938/6938 [==============================] - 1s 85us/step - loss: 3.6437 - categorical_accuracy: 0.0817\n",
      "Epoch 10/22\n",
      "6938/6938 [==============================] - 1s 86us/step - loss: 3.4703 - categorical_accuracy: 0.1061\n",
      "Epoch 11/22\n",
      "6938/6938 [==============================] - 1s 88us/step - loss: 3.2747 - categorical_accuracy: 0.1434\n",
      "Epoch 12/22\n",
      "6938/6938 [==============================] - 1s 85us/step - loss: 3.0606 - categorical_accuracy: 0.1692\n",
      "Epoch 13/22\n",
      "6938/6938 [==============================] - 1s 87us/step - loss: 2.8497 - categorical_accuracy: 0.2175\n",
      "Epoch 14/22\n",
      "6938/6938 [==============================] - 1s 87us/step - loss: 2.6597 - categorical_accuracy: 0.2610\n",
      "Epoch 15/22\n",
      "6938/6938 [==============================] - 1s 88us/step - loss: 2.4449 - categorical_accuracy: 0.3071\n",
      "Epoch 16/22\n",
      "6938/6938 [==============================] - 1s 89us/step - loss: 2.2640 - categorical_accuracy: 0.3553\n",
      "Epoch 17/22\n",
      "6938/6938 [==============================] - 1s 88us/step - loss: 2.0588 - categorical_accuracy: 0.4005\n",
      "Epoch 18/22\n",
      "6938/6938 [==============================] - 1s 88us/step - loss: 1.8653 - categorical_accuracy: 0.4552\n",
      "Epoch 19/22\n",
      "6938/6938 [==============================] - 1s 89us/step - loss: 1.7086 - categorical_accuracy: 0.4970\n",
      "Epoch 20/22\n",
      "6938/6938 [==============================] - 1s 88us/step - loss: 1.5458 - categorical_accuracy: 0.5458\n",
      "Epoch 21/22\n",
      "6938/6938 [==============================] - 1s 89us/step - loss: 1.3966 - categorical_accuracy: 0.5895\n",
      "Epoch 22/22\n",
      "6938/6938 [==============================] - 1s 93us/step - loss: 1.2883 - categorical_accuracy: 0.6189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f70ac08bb70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_logger = CSVLogger('training1.log')\n",
    "model1.fit(x_train, y_train, epochs = 22, batch_size = 128, callbacks = [csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "x = ingred\n",
    "calories = h_val[:, 2]\n",
    "protein = h_val[:, 3]\n",
    "fat = h_val[:, 4]\n",
    "y = protein / (calories * fat)\n",
    "print(np.where(y == 0))\n",
    "idx = np.argsort(y)\n",
    "idx = idx[:-250]\n",
    "np.random.shuffle(idx)\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "maxi = np.max(y)\n",
    "y = y / maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 200)               61800     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 500)               100500    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 1,164,801\n",
      "Trainable params: 1,164,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential([Dense(200, input_shape=(308,)),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(500),\n",
    "                                    LeakyReLU(),\n",
    "                                    \n",
    "                                    Dense(500),\n",
    "                                    LeakyReLU(),\n",
    "                    \n",
    "                                    Dense(500),\n",
    "                                    LeakyReLU(),\n",
    "                    \n",
    "                                    Dense(500),\n",
    "                                    LeakyReLU(),\n",
    "                    \n",
    "                                    Dense(500),\n",
    "                                    LeakyReLU(),\n",
    "                                                        \n",
    "                                    Dense(1)\n",
    "                                   ])\n",
    "model2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/22\n",
      "1909/1909 [==============================] - 0s 110us/step - loss: 0.0018\n",
      "Epoch 2/22\n",
      "1909/1909 [==============================] - 0s 112us/step - loss: 0.0018\n",
      "Epoch 3/22\n",
      "1909/1909 [==============================] - 0s 106us/step - loss: 0.0018\n",
      "Epoch 4/22\n",
      "1909/1909 [==============================] - 0s 113us/step - loss: 0.0017\n",
      "Epoch 5/22\n",
      "1909/1909 [==============================] - 0s 113us/step - loss: 0.0017\n",
      "Epoch 6/22\n",
      "1909/1909 [==============================] - 0s 118us/step - loss: 0.0017\n",
      "Epoch 7/22\n",
      "1909/1909 [==============================] - 0s 113us/step - loss: 0.0018\n",
      "Epoch 8/22\n",
      "1909/1909 [==============================] - 0s 121us/step - loss: 0.0019\n",
      "Epoch 9/22\n",
      "1909/1909 [==============================] - 0s 115us/step - loss: 0.0022\n",
      "Epoch 10/22\n",
      "1909/1909 [==============================] - 0s 115us/step - loss: 0.0020\n",
      "Epoch 11/22\n",
      "1909/1909 [==============================] - 0s 113us/step - loss: 0.0019\n",
      "Epoch 12/22\n",
      "1909/1909 [==============================] - 0s 119us/step - loss: 0.0019\n",
      "Epoch 13/22\n",
      "1909/1909 [==============================] - 0s 116us/step - loss: 0.0018\n",
      "Epoch 14/22\n",
      "1909/1909 [==============================] - 0s 114us/step - loss: 0.0018\n",
      "Epoch 15/22\n",
      "1909/1909 [==============================] - 0s 118us/step - loss: 0.0018\n",
      "Epoch 16/22\n",
      "1909/1909 [==============================] - 0s 114us/step - loss: 0.0018\n",
      "Epoch 17/22\n",
      "1909/1909 [==============================] - 0s 118us/step - loss: 0.0018\n",
      "Epoch 18/22\n",
      "1909/1909 [==============================] - 0s 116us/step - loss: 0.0018\n",
      "Epoch 19/22\n",
      "1909/1909 [==============================] - 0s 133us/step - loss: 0.0017\n",
      "Epoch 20/22\n",
      "1909/1909 [==============================] - 0s 147us/step - loss: 0.0018\n",
      "Epoch 21/22\n",
      "1909/1909 [==============================] - 0s 116us/step - loss: 0.0017\n",
      "Epoch 22/22\n",
      "1909/1909 [==============================] - 0s 125us/step - loss: 0.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f7059f63cf8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_logger = CSVLogger('training2.log')\n",
    "model2.fit(x, y, epochs = 22, batch_size=1024, callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5354466858789625\n"
     ]
    }
   ],
   "source": [
    "cnt1 = 0\n",
    "cnt2 = 0\n",
    "for i in range(x_test.shape[0]):\n",
    "    x = x_test[i].reshape(1, x_test.shape[1])\n",
    "    y = model1.predict(x, batch_size = 128)\n",
    "    y_idx = np.argsort(y[0])\n",
    "    y_idx = y_idx[-20:]\n",
    "    if np.where(y_test[i] == 1) in y_idx:\n",
    "        cnt1 += 1\n",
    "    cnt2 += 1\n",
    "print(cnt1 / cnt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9835687518016719\n"
     ]
    }
   ],
   "source": [
    "cnt1 = 0\n",
    "cnt2 = 0\n",
    "for i in range(x_train.shape[0]):\n",
    "    x = x_train[i].reshape(1, x_train.shape[1])\n",
    "    y = model1.predict(x, batch_size = 128)\n",
    "    y_idx = np.argsort(y[0])\n",
    "    y_idx = y_idx[-20:]\n",
    "    if np.where(y_train[i] == 1) in y_idx:\n",
    "        cnt1 += 1\n",
    "    cnt2 += 1\n",
    "print(cnt1 / cnt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(x_idx):\n",
    "    x = np.zeros((1, ingred.shape[1]))\n",
    "    x[0, x_idx] = 1\n",
    "    y = model1.predict(x, batch_size = 128)\n",
    "    y_idx = np.argsort(y[0])\n",
    "    y_idx = y_idx[-5:]\n",
    "    print('Initial Suggestions: ')\n",
    "    print(keptcolumns[y_idx + 6])\n",
    "    print('Scores: ')\n",
    "    print(y[0][y_idx])\n",
    "    h_val = np.zeros(5)\n",
    "    for i in range(5):\n",
    "        x_tmp = np.append(x_idx, y_idx[i])\n",
    "        x = np.zeros((1, 308))\n",
    "        x[0, x_tmp] = 1        \n",
    "        y_tmp = model2.predict(x, batch_size = 128)\n",
    "        h_val[i] = y_tmp\n",
    "    y_idx_idx = np.argsort(h_val)\n",
    "    y_idx = y_idx[y_idx_idx]\n",
    "    print('Final Suggestions Based on Health: ')\n",
    "    print(keptcolumns[y_idx + 6])\n",
    "    print('Health values: ')\n",
    "    print(h_val[y_idx_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe name:  Asian Noodles with Barbecued Duck Confit \n",
      "Original Ingredients: \n",
      "[['carrot' 'duck' 'green bean' 'noodle' 'soy sauce' 'vinegar']]\n",
      "Initial Suggestions: \n",
      "['fennel' 'radish' 'lentil' 'rice' 'noodle']\n",
      "Scores: \n",
      "[0.00085223 0.00102717 0.00204197 0.00388891 0.08549684]\n",
      "Final Suggestions Based on Health: \n",
      "['noodle' 'lentil' 'fennel' 'radish' 'rice']\n",
      "Health values: \n",
      "[0.12778445 0.33051831 0.33354715 0.47515813 0.48013535]\n"
     ]
    }
   ],
   "source": [
    "rec_no = 78\n",
    "print(\"Recipe name: \", h_val[rec_no][0])\n",
    "idx = np.array(np.where(ingred[rec_no] == 1))\n",
    "# print(idx)\n",
    "print(\"Original Ingredients: \")\n",
    "print(keptcolumns[idx + 6])\n",
    "test_model(np.array([49, 93, 118, 263, 294]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
